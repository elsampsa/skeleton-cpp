<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>skeleton: Library architecture</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">skeleton
   &#160;<span id="projectnumber">0.3.0</span>
   </div>
   <div id="projectbrief">skeleton</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Library architecture </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>H264 decoding is done on the CPU, using the FFmpeg library.</p>
<p>The final operation of interpolation from YUV bitmap into a RGB bitmap of correct (window) size is done on the GPU, using the openGL shading language.</p>
<p>This approach is a nice compromise as it takes some advantage of the GPU by offloading the (heavy) interpolation of (large) bitmaps.</p>
<p>One might think that it would be fancier to do the H264 decoding on the GPU as well, but this is a road to hell - forget about it.</p>
<p>Nv$dia for example, offers H264/5 decoding directly on their GPUs, but then you are dependent on their proprietary implementation, which means that:</p>
<ul>
<li>You'll never know how many H264 streams the proprietary graphics drivers is allowed to decode simultaneously. Such restraints are completely artificial and they are implemented so that you would buy a more expensive "specialized" card.</li>
<li>You'll never know what H264 "profiles" the proprietary driver supports.</li>
<li>There is no way you can even find out these things - no document exists that would reveal which chipset/card supports how many simultaneous H264 streams and which H264 profiles.</li>
</ul>
<p>So, if you're decoding only one H264 stream, then it might be ok to use proprietary H264 decoding on the GPU (but on the other hand, what's the point if it's only one stream..). If you want to do some serious parallel streaming (like here), invest on CPUs instead.</p>
<p>Other possibilities to transfer the H264 decoding completely to the GPU are (not implemented in Valkka at the moment):</p>
<ul>
<li>Use Nvidia VDPAU (the API is open source) of the Mesa stack. In this case you must use X.org drivers for your GPU.</li>
<li>Create a H264 decoder based on the OpenGL shading language. This would be cool (and demanding) project.</li>
</ul>
<p>In Valkka, concurrency in decoding and presenting various streams simultaneously is achieved using multithreading and mutex-protected fifos. This works roughly as follows:</p>
<pre class="fragment">Live555 thread (LiveThread)     FrameFifo       Decoding threads      OpenGLFrameFifo          OpenGLThread
                                                     
                                                                                              +-------------+ 
                                                                                              |             |
 +---------------------------+                                                                |interpolation|
 | rtsp negotiation          | -&gt; [FIFO] -&gt;      [AVThread] -&gt;                                |timing       |
 | frame composition         | -&gt; [FIFO] -&gt;      [AVthread] -&gt;          [Global FIFO] -&gt;      |presentation |
 |                           | -&gt; [FIFO] -&gt;      [AVthread] -&gt;                                |             |
 +---------------------------+                                                                |             |
                                                                                              +-------------+
</pre><p>A general purpose "mother" class Thread has been implemented (see <a class="el" href="group__threading__tag.html">multithreads</a>) for multithreading schemes and is inherited by:</p>
<ul>
<li>LiveThread, for connecting to media sources using the Live555 streaming library, see <a class="el" href="group__livethread__tag.html">livethread</a></li>
<li>AVThread, for decoding streams using the FFMpeg library and uploading them to GPU, see <a class="el" href="group__decoding__tag.html">decoding</a></li>
<li>OpenGLThread, that handles direct memory access to GPU and presents the Frames, based on their timestamps, see <a class="el" href="group__openglthread__tag.html">opengl</a></li>
</ul>
<p>To get a rough idea how Live555 works, please see <a class="el" href="live555_page.html">Live555 primer</a> and <a class="el" href="group__live__tag.html">live555 bridge</a>. The livethread produces frames (class Frame), that are passed to mutex-protected fifos (see <a class="el" href="group__queues__tag.html">queues and fifos</a>).</p>
<p>Between the threads, frames are passed through series of "filters" (see <a class="el" href="group__filters__tag.html">available framefilters</a>). Filters can be used to modify the media packets (say, their timestamps for example) and to produce copying and redirection of the stream. Valkka library filters should not be confused with Live555 sink/source/filters nor with FFmpeg filters - which are completely different things.</p>
<p>For visualization of the media stream plumbings / graphs, we adopt the following notation which you should always use when commenting your python or cpp code:</p>
<pre class="fragment">() == Thread
{} == FrameFilter
[] == FrameFifo queue
</pre><p>To be more informative, we use:</p>
<pre class="fragment">(N. Thread class name: variable name)
{N. FrameFilter class name: variable name)
[N. FrameFifo class name: variable name]
</pre><p>A typical thread / framefilter graph would then look like this:</p>
<pre class="fragment">(1.LiveThread:livethread) --&gt; {2.TimestampFrameFilter:myfilter} --&gt; {3.FifoFrameFilter:fifofilter} --&gt; [4.FrameFifo:framefifo] --&gt;&gt; (5.AVThread:avthread) --&gt; ...
</pre><p> Which means that ..</p><ul>
<li>(1) LiveThread reads the rtsp camera source, passes the frames to filter (2) that corrects the timestamp of the frame.</li>
<li>(2) passes the frames to a special filter (FifoFrameFilter) which feeds a fifo queue (4).</li>
<li>(4) FrameFifo is a class that handles a mutex-proteced fifo and a stack for frames</li>
</ul>
<p>The whole filter chain from (1) to (4) is simply a callback cascade. Because of this, the execution of LiveThread (1) is blocked, until the callback chain has been completed. The callback chain ends to the "thread border", marked with "--&gt;&gt;". On the "other side" of the thread border, another thread is running independently.</p>
<p>Also, keep in mind the following rule:</p>
<ul>
<li>Processes read from mutex-protected fifos (base class for fifos is FrameFifo)</li>
<li>Processes write into filters (base class FrameFilter)</li>
</ul>
<p>In the case of LiveThread, the API user passes a separate FrameFilter per each requested stream to LiveThread. That FrameFilter then serves as a starting point for the filter chain. The last filter in the chain is typically FifoFrameFilter, e.g. a filter that feeds the (modified/filtered) decoded frame to a fifo that is then being consumed by AVThread.</p>
<p>For more details, refer to examples, doxygen documentation and the source code itself.</p>
<p>Remember that example we sketched in the github readme page? Using our notation, it would look like this:</p>
<pre class="fragment"> (1.LiveThread:livethread) --&gt; {2.TimestampFrameFilter:myfilter} 
                                  |
                                  +--&gt; {3.ForkFrameFilter:forkfilter}  
                                         |    |
                                         |    |
       through filters, to filesystem &lt;--+    +--&gt; {4.FifoFrameFilter:fifofilter} --&gt; [5.FrameFifo:framefifo] --&gt;&gt; (6.AVThread:avthread)
                                                                                                                             |
                                                                                                                             |
                                                                                                                             +--&gt; {7.ForkFrameFilter:forkfilter2}  
                                                                                                                                           |    |                                                                                    
                                                                                                                                           |    |
                                  (10.OpenGLThread:openglthread) &lt;&lt;-- [9.OpenGLFrameFifo:gl_fifo] &lt;-- {8.FifoFrameFilter:gl_in_filter}  &lt;--+    +--&gt; .. finally, to analyzing process
                             feeds the video into various X-windoses</pre><ul>
<li>For the various FrameFilters, see <a class="el" href="group__filters__tag.html">available framefilters</a></li>
<li>For threads, see <a class="el" href="group__threading__tag.html">multithreads</a></li>
</ul>
<p>Some more miscellaneous details about the architecture:</p>
<ul>
<li>AVThread decoding threads both decode (with FFmpeg) and runs the uploading of the YUV bitmaps to the GPU. Uploading pixel buffer objects takes place in OpenGLFrameFifo.</li>
<li>The OpenGLThread thread performs the final interpolation from YUV into RGB bitmap using the openGL shading language</li>
<li>Fifos are a combination of a fifo queue and a stack: each frame inserted into the fifo is taken from an internal reservoir stack. If no frames are left in the stack, it means overflow and the fifo/stack is resetted to its initial state</li>
<li>Reserving items for the fifo <em>beforehand</em> and placing them into a reservoir stack avoids constant memory (de)allocations that can become a bottleneck in multithreading schemes.</li>
<li>This way we also get "graceful overflow" behaviour: in the case the decoding threads or the OpenGL thread being too slow (i.e. if you have too many/heavy streams), the pipeline overflows in a controlled way.</li>
<li>For a complete walk-through from stream source to x window, check out <a class="el" href="pipeline.html">Code walkthrough: rendering</a> </li>
</ul>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
</body>
</html>
